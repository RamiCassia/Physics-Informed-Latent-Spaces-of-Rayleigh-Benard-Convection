{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PILS-CAE.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPCL0EAOXVDx9TKrSvTF0U9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"JIM7o6DZUPyF"},"outputs":[],"source":["from google.colab import drive\n","drive.mount(\"/content/gdrive\")\n","\n","import time\n","import os\n","import glob\n","import random\n","import json\n","import subprocess\n","import sys\n","import gc\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from collections import OrderedDict\n","import xml.etree.ElementTree\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","def set_deterministic():\n","    if torch.cuda.is_available():\n","        torch.backends.cudnn.benchmark = False\n","        torch.backends.cudnn.deterministic = True\n","    torch.set_deterministic(True)\n","    \n","    \n","def set_all_seeds(seed):\n","    os.environ[\"PL_GLOBAL_SEED\"] = str(seed)\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","\n","    def compute_accuracy(model_diff, model_conv, data_loader_diff, data_loader_conv, data_loader_base, device):\n","    model_diff.eval()\n","    model_conv.eval()\n","    numers = []\n","    denoms = []\n","    with torch.no_grad():\n","  \n","        for (features_diff, _), (features_conv, _), (_, targets_base) in zip(data_loader_diff, data_loader_conv, data_loader_base):\n","\n","            targets_base = targets_base.to(device)\n","\n","            features_diff = features_diff.to(device)\n","            features_conv = features_conv.to(device)\n","\n","            recon = model_diff(features_diff) + model_conv(features_conv)\n","\n","            numer = np.sum(np.square(np.array(targets_base.cpu()) - np.array(recon.cpu())))\n","            numers = numers + [numer]\n","\n","            denom = np.sum(np.square(np.array(targets_base.cpu())))\n","            denoms = denoms + [denom]\n","\n","    acc = (1 - (np.array(np.ravel(numers))/np.array(np.ravel(denoms))))\n","\n","    return np.round(acc[0], 5)\n","    \n","\n","def compute_epoch_loss_autoencoder(model_diff, model_conv, data_loader_diff, data_loader_conv, data_loader_base, loss_fn, device):\n","    model.eval()\n","    curr_loss, num_examples = 0., 0\n","    with torch.no_grad():\n","        for (features_diff, _), (features_conv, _), (_, targets_base) in zip(data_loader_diff, data_loader_conv, data_loader_base):\n","\n","            features_diff = features_diff.to(device)\n","            features_conv = features_conv.to(device)\n","            targets_base = targets_base.to(device)\n","\n","            predictions = model_diff(features_diff) + model_conv(features_conv)\n","            loss = loss_fn(predictions, targets_base, reduction='sum')\n","            num_examples += targets_base.size(0)\n","            curr_loss += loss\n","\n","            features_diff = features_diff.to('cpu')\n","            features_conv = features_conv.to('cpu')\n","            targets_base = targets_base.to('cpu')\n","\n","            del features_diff\n","            del features_conv\n","            del targets_base\n","\n","        curr_loss = curr_loss / num_examples\n","        return curr_loss\n","\n","\n","def train_autoencoder_v1(num_epochs, model_diff, model_conv, optimizer_diff, optimizer_conv, \n","                         train_loader_diff, train_loader_conv, train_loader_base, device, loss_fn=None, \n","                         skip_epoch_stats=False,\n","                         save_model=None):\n","    \n","    log_dict = {'train_loss_per_batch': [],\n","                'train_loss_per_epoch': []}\n","    \n","    if loss_fn is None:\n","        loss_fn = F.mse_loss\n","\n","    start_time = time.time()\n","    for epoch in range(num_epochs):\n","\n","        model_diff.train()\n","        model_conv.train()\n","\n","        batch_idx = 0\n","\n","        for (features_diff, _), (features_conv, _), (_, targets_base) in zip(train_loader_diff, train_loader_conv, train_loader_base):\n","\n","            batch_idx += 1\n","\n","            features_diff = features_diff.to(device)\n","            features_conv = features_conv.to(device)\n","            targets_base = targets_base.to(device)\n","\n","            # FORWARD AND BACK PROP\n","            predictions = model_diff(features_diff) + model_conv(features_conv)\n","            loss = loss_fn(predictions, targets_base)\n","            optimizer_diff.zero_grad()\n","            optimizer_conv.zero_grad()\n","\n","\n","            loss.backward()\n","\n","            # UPDATE MODEL PARAMETERS\n","            optimizer_diff.step()\n","            optimizer_conv.step()\n","\n","            #additional\n","\n","            features_diff = features_diff.to('cpu')\n","            features_conv = features_conv.to('cpu')\n","            targets_base = targets_base.to('cpu')\n","\n","            del features_diff\n","            del features_conv\n","            del targets_base\n","\n","            # LOGGING\n","            log_dict['train_loss_per_batch'].append(loss.item())\n","            \n","            print('Epoch: %03d/%03d | Batch %04d/%04d | Loss: %.4f'\n","                  % (epoch+1, num_epochs, batch_idx,\n","                       len(train_loader_diff), loss))\n","\n","        if not skip_epoch_stats:\n","            model_diff.eval()\n","            model_conv.eval()\n","            \n","            with torch.set_grad_enabled(False):  # save memory during inference\n","                \n","                train_loss = compute_epoch_loss_autoencoder(\n","                    model_diff, model_conv, train_loader_diff, train_loader_conv, train_loader_base, loss_fn, device)\n","                print('***Epoch: %03d/%03d | Loss: %.3f' % (\n","                      epoch+1, num_epochs, train_loss))\n","                log_dict['train_loss_per_epoch'].append(train_loss.item())\n","\n","        print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n","\n","    print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))\n","    if save_model is not None:\n","        torch.save(model.state_dict(), save_model)\n","    \n","    return log_dict\n","\n","\n","\n","def plot_training_loss(minibatch_losses, num_epochs, averaging_iterations=100, custom_label=''):\n","\n","    iter_per_epoch = len(minibatch_losses) // num_epochs\n","\n","    plt.figure()\n","    ax1 = plt.subplot(1, 1, 1)\n","    ax1.plot(range(len(minibatch_losses)),\n","             (minibatch_losses), label=f'Minibatch Loss{custom_label}')\n","    ax1.set_xlabel('Iterations')\n","    ax1.set_ylabel('Loss')\n","\n","    if len(minibatch_losses) < 1000:\n","        num_losses = len(minibatch_losses) // 2\n","    else:\n","      num_losses = 1000\n","\n","    #ax1.set_ylim([\n","    #    0, np.max(minibatch_losses[num_losses:])*1.5\n","    #    ])\n","\n","    ax1.plot(np.convolve(minibatch_losses,\n","                         np.ones(averaging_iterations,)/averaging_iterations,\n","                         mode='valid'),\n","             label=f'Running Average{custom_label}')\n","    ax1.legend()\n","\n","def plot_accuracy(train_acc, valid_acc):\n","\n","    num_epochs = len(train_acc)\n","\n","    plt.plot(np.arange(1, num_epochs+1), \n","             train_acc, label='Training')\n","    plt.plot(np.arange(1, num_epochs+1),\n","             valid_acc, label='Validation')\n","\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Accuracy')\n","    plt.legend()\n","\n","\n","\n","def scientific_to_string(Ra):\n","  Ra_s = format(Ra, '.3e')\n","  Ra_s = Ra_s.replace('.', 'p')\n","  Ra_s = Ra_s.replace('+', '')\n","\n","  return Ra_s\n","\n","def create_result_dirs(Rai, Raf, ti, tf):\n","\n","  Rai_s = scientific_to_string(Rai)\n","  Raf_s = scientific_to_string(Raf)\n","\n","  main_directory = Rai_s + '_' + Raf_s + '_' + str(ti) + '_' + str(tf)\n","\n","  root_path = '/content/gdrive/My Drive/Project/Results/'\n","\n","  os.mkdir(os.path.join(root_path, main_directory))\n","\n","  snapshots_folder = ['snapshots', 'snapshots/' + str(ti) + '_0', 'snapshots/' + str(ti) + '_0' + '/arrays', 'snapshots/' + str(ti) + '_0' + '/images', 'snapshots/' + str(tf) + '_1', 'snapshots/' + str(tf) + '_1' + '/arrays', 'snapshots/' + str(tf) + '_1' + '/images']\n","\n","  segmentations_folder = ['segmentations', 'segmentations/' + str(ti) + '_0', 'segmentations/' + str(ti) + '_0' + '/arrays', 'segmentations/' + str(ti) + '_0' + '/images', 'segmentations/' + str(tf) + '_1', 'segmentations/' + str(tf) + '_1' + '/arrays', 'segmentations/' + str(tf) + '_1' + '/images']\n","\n","  masks_folder = ['masks', 'masks/diffusion', 'masks/diffusion/' + str(ti) + '_0', 'masks/diffusion/' + str(ti) + '_0' + '/arrays', 'masks/diffusion/' + str(ti) + '_0' + '/images', 'masks/diffusion/' + str(tf) + '_1', 'masks/diffusion/' + str(tf) + '_1' + '/arrays', 'masks/diffusion/' + str(tf) + '_1' + '/images', 'masks/convection' ,'masks/convection/' + str(ti) + '_0', 'masks/convection/' + str(ti) + '_0' + '/arrays', 'masks/convection/' + str(ti) + '_0' + '/images', 'masks/convection/' + str(tf) + '_1', 'masks/convection/' + str(tf) + '_1' + '/arrays', 'masks/convection/' + str(tf) + '_1' + '/images']\n","\n","  models_folder = ['models', 'models/diffusion', 'models/convection', 'models/base', 'models/diff_conv_casc']\n","\n","  folders = snapshots_folder + segmentations_folder + masks_folder + models_folder\n","\n","  for folder in folders:\n","    os.mkdir(os.path.join(os.path.join(root_path, main_directory), folder))\n","\n","  return Rai_s, Raf_s\n","\n","\n","def load_data(Rai, Raf, ti, tf):\n","\n","  file_count = 0\n","\n","  _, _, files = next(os.walk('/content/gdrive/My Drive/Project/Results/' + Rai + '_' + Raf + '_13_1_19' + '/snapshots/' + str(ti) + '/arrays'))\n","  file_count = len(files)\n","\n","  data_x_diff = []\n","  file_list_x_diff = []\n","\n","  data_x_conv = []\n","  file_list_x_conv = []\n","\n","  data_y = []\n","  file_list_y = []\n","\n","  for i in range(file_count):\n","\n","    if i > 2140: \n","      continue  \n","\n","    file_e_diff = glob.glob('/content/gdrive/My Drive/Project/Results/' + Rai + '_' + Raf + '_13_1_19' + '/masks/diffusion/' + str(ti) + '/arrays/' + str(i) + '_' + '*.npy')\n","    file_list_x_diff = file_list_x_diff + [file_e_diff]\n","\n","    file_e_conv = glob.glob('/content/gdrive/My Drive/Project/Results/' + Rai + '_' + Raf + '_13_1_19' + '/masks/convection/' + str(ti) + '/arrays/' + str(i) + '_' + '*.npy')\n","    file_list_x_conv = file_list_x_conv + [file_e_conv]\n","\n","    file_e_y = glob.glob('/content/gdrive/My Drive/Project/Results/' + Rai + '_' + Raf + '_13_1_19'  + '/snapshots/' + str(tf) + '/arrays/' + str(i) + '_' + '*.npy')\n","    file_list_y = file_list_y + [file_e_y]\n","\n","\n","  file_list_x_diff = np.ravel(file_list_x_diff)\n","  file_list_x_conv = np.ravel(file_list_x_conv)\n","  file_list_y = np.ravel(file_list_y)\n","\n","  for file_path in file_list_x_diff:\n","    \n","    a = np.load(file_path, allow_pickle=True)\n","\n","    data_x_diff = data_x_diff + [a]\n","\n","\n","  for file_path in file_list_x_conv:\n","    \n","    a = np.load(file_path, allow_pickle=True)\n","\n","    data_x_conv = data_x_conv + [a]  \n","\n","\n","  for file_path in file_list_y:\n","    \n","    a = np.load(file_path, allow_pickle=True)\n","\n","    data_y = data_y + [a]\n","  \n","  return data_x_diff, data_x_conv, data_y\n","\n","\n","def print_files(Rai, Raf, ti, tf):\n","\n","  file_count = 0\n","\n","  _, _, files = next(os.walk('/content/gdrive/My Drive/Project/Results/' + Rai + '_' + Raf + '_13_1_19' + '/snapshots/' + str(ti) + '/arrays'))\n","  file_count = len(files)\n","\n","  data_x_diff = []\n","  file_list_x_diff = []\n","\n","  data_x_conv = []\n","  file_list_x_conv = []\n","\n","  data_y = []\n","  file_list_y = []\n","\n","  for i in range(file_count):\n","\n","    if i > 2140: \n","      continue  \n","\n","    file_e_diff = glob.glob('/content/gdrive/My Drive/Project/Results/' + Rai + '_' + Raf + '_13_1_19' + '/masks/diffusion/' + str(ti) + '/arrays/' + str(i) + '_' + '*.npy')\n","    file_list_x_diff = file_list_x_diff + [file_e_diff]\n","\n","    file_e_conv = glob.glob('/content/gdrive/My Drive/Project/Results/' + Rai + '_' + Raf + '_13_1_19' + '/masks/convection/' + str(ti) + '/arrays/' + str(i) + '_' + '*.npy')\n","    file_list_x_conv = file_list_x_conv + [file_e_conv]\n","\n","    file_e_y = glob.glob('/content/gdrive/My Drive/Project/Results/' + Rai + '_' + Raf + '_13_1_19'  + '/snapshots/' + str(tf) + '/arrays/' + str(i) + '_' + '*.npy')\n","    file_list_y = file_list_y + [file_e_y]\n","\n","\n","  file_list_x_diff = np.ravel(file_list_x_diff)\n","  file_list_x_conv = np.ravel(file_list_x_conv)\n","  file_list_y = np.ravel(file_list_y)\n","\n","  return file_list_x_diff, file_list_x_conv, file_list_y\n","\n","\n","def split_files(file_list_x_diff, file_list_x_conv, file_list_y, train_size, seed = 0):\n","\n","  file_list_x_diff = random.Random(seed).sample(list(file_list_x_diff), len(file_list_x_diff))\n","  file_list_x_conv = random.Random(seed).sample(list(file_list_x_conv), len(file_list_x_conv))\n","  file_list_y = random.Random(seed).sample(list(file_list_y), len(file_list_y))\n","\n","\n","  middle_index = int(np.round(len(file_list_x_diff) * train_size))\n","\n","  #file_list_x_train_diff = file_list_x_diff[:middle_index]\n","  file_list_x_test_diff = file_list_x_diff[middle_index:]\n","\n","  #file_list_x_train_conv = file_list_x_conv[:middle_index]\n","  file_list_x_test_conv = file_list_x_conv[middle_index:]\n","\n","  #file_list_y_train = file_list_y[:middle_index]\n","  file_list_y_test = file_list_y[middle_index:]\n","\n","  return file_list_x_test_diff, file_list_x_test_conv, file_list_y_test\n","\n","\n","def swap(i,j):\n","  a = i.swapaxes(0,1)\n","  b = j.swapaxes(0,1)\n","  return a,b\n","\n","def unsqueeze(i,j):\n","  a = torch.unsqueeze(i,0)\n","  b = torch.unsqueeze(j,0)\n","  return a,b\n","\n","\n","def split_x_y_list(data_x_diff, data_x_conv, data_y, train_size, seed):\n","\n","\n","  data_x_diff = random.Random(seed).sample(data_x_diff, len(data_x_diff))\n","  data_x_conv = random.Random(seed).sample(data_x_conv, len(data_x_conv))\n","  data_y = random.Random(seed).sample(data_y, len(data_y))\n","\n","\n","  middle_index = int(np.round(len(data_x_diff) * train_size))\n","\n","  data_x_train_diff = data_x_diff[:middle_index]\n","  data_x_test_diff = data_x_diff[middle_index:]\n","\n","  data_x_train_conv = data_x_conv[:middle_index]\n","  data_x_test_conv = data_x_conv[middle_index:]\n","\n","  data_y_train = data_y[:middle_index]\n","  data_y_test = data_y[middle_index:]\n","\n","  return data_x_train_diff, data_x_train_conv, data_y_train, data_x_test_diff, data_x_test_conv, data_y_test\n","\n","\n","def generate_loader(data_x_diff, data_x_conv, data_y, TRAIN_SIZE, BATCH_SIZE, SEED):\n","\n","  data_x_train_diff, data_x_train_conv, data_y_train, data_x_test_diff, data_x_test_conv, data_y_test = split_x_y_list(data_x_diff, data_x_conv, data_y, TRAIN_SIZE, SEED)\n","\n","  ##########################\n","  ### Dataset\n","  ##########################\n","\n","  import torch\n","  import numpy as np\n","  from torch.utils.data import TensorDataset, DataLoader\n","\n","  import random\n","\n","  my_x_train_diff = np.array(data_x_train_diff)\n","  my_x_test_diff = np.array(data_x_test_diff)\n","\n","  my_x_train_conv = np.array(data_x_train_conv)\n","  my_x_test_conv = np.array(data_x_test_conv)\n","\n","  my_y_train = np.array(data_y_train) \n","  my_y_test = np.array(data_y_test)\n","\n","  tensor_x_train_diff = torch.Tensor(my_x_train_diff) # transform to torch tensor\n","  tensor_x_test_diff = torch.Tensor(my_x_test_diff)\n","  tensor_x_train_conv = torch.Tensor(my_x_train_conv) # transform to torch tensor\n","  tensor_x_test_conv = torch.Tensor(my_x_test_conv)\n","  tensor_y_train = torch.Tensor(my_y_train) \n","  tensor_y_test = torch.Tensor(my_y_test)\n","\n","  #tensor_x_train, tensor_x_test, tensor_y_train, tensor_y_test = tensor_x_train.to(DEVICE), tensor_x_test.to(DEVICE), tensor_y_train.to(DEVICE), tensor_y_test.to(DEVICE)\n","\n","  train_dataset_diff = TensorDataset(tensor_x_train_diff, tensor_y_train)\n","  test_dataset_diff = TensorDataset(tensor_x_test_diff, tensor_y_test)\n","\n","  train_dataset_conv = TensorDataset(tensor_x_train_conv, tensor_y_train)\n","  test_dataset_conv = TensorDataset(tensor_x_test_conv, tensor_y_test)  \n","\n","  train_loader_diff = DataLoader(train_dataset_diff, batch_size = BATCH_SIZE) \n","  test_loader_diff = DataLoader(test_dataset_diff, batch_size = BATCH_SIZE)\n","\n","  train_loader_conv = DataLoader(train_dataset_conv, batch_size = BATCH_SIZE) \n","  test_loader_conv = DataLoader(test_dataset_conv, batch_size = BATCH_SIZE)  \n","\n","\n","  train_loader_diff = [unsqueeze(i,j) for (i,j) in train_loader_diff]\n","  train_loader_diff = [swap(i,j) for (i,j) in train_loader_diff]\n","\n","  test_loader_diff = [unsqueeze(i,j) for (i,j) in test_loader_diff]\n","  test_loader_diff = [swap(i,j) for (i,j) in test_loader_diff]\n","\n","  train_loader_conv = [unsqueeze(i,j) for (i,j) in train_loader_conv]\n","  train_loader_conv = [swap(i,j) for (i,j) in train_loader_conv]\n","\n","  test_loader_conv = [unsqueeze(i,j) for (i,j) in test_loader_conv]\n","  test_loader_conv = [swap(i,j) for (i,j) in test_loader_conv]\n","\n","  # Checking the dataset\n","  print('Training Set:\\n')\n","  for image_x, image_y in train_loader_diff: \n","      print('Image x batch dimensions:', image_x.size())\n","      print('Image y batch dimensions:', image_y.size())\n","      break\n","\n","  # Checking the dataset\n","  print('\\nTesting Set:')\n","  for image_x, image_y in test_loader_diff:\n","      print('Image x batch dimensions:', image_x.size())\n","      print('Image y batch dimensions:', image_y.size())\n","      break\n","\n","  print('Training Set:\\n')\n","  for image_x, image_y in train_loader_conv: \n","      print('Image x batch dimensions:', image_x.size())\n","      print('Image y batch dimensions:', image_y.size())\n","      break\n","\n","  # Checking the dataset\n","  print('\\nTesting Set:')\n","  for image_x, image_y in test_loader_conv:\n","      print('Image x batch dimensions:', image_x.size())\n","      print('Image y batch dimensions:', image_y.size())\n","      break\n","\n","  return train_loader_diff, test_loader_diff, train_loader_conv, test_loader_conv\n","\n","\n","\n","##########################\n","### MODEL\n","##########################\n","\n","\n","class Reshape(nn.Module):\n","    def __init__(self, *args):\n","        super().__init__()\n","        self.shape = args\n","\n","    def forward(self, x):\n","        return x.view(self.shape)\n","\n","\n","class Trim(nn.Module):\n","    def __init__(self, *args):\n","        super().__init__()\n","\n","    def forward(self, x):\n","        return x[:, :, :256, :256]\n","\n","\n","class AutoEncoder(nn.Module):\n","\n","    def __init__(self, m):\n","        super().__init__()\n","        \n","        self.encoder = nn.Sequential( #784\n","                nn.Conv2d(1, 4, stride=(1, 1), kernel_size=(4, 4), padding=0),\n","                nn.LeakyReLU(0.01),\n","                nn.Conv2d(4, 16, stride=(1, 1), kernel_size=(4, 4), padding=0),\n","                nn.LeakyReLU(0.01),\n","                nn.Conv2d(16, 16, stride=(1, 1), kernel_size=(4, 4), padding=0),\n","                nn.LeakyReLU(0.01),\n","                nn.Conv2d(16, 16, stride=(1, 1), kernel_size=(4, 4), padding=0),\n","                nn.Flatten(),\n","                nn.Linear(952576, m)\n","                )\n","\n","        self.decoder = nn.Sequential(\n","                torch.nn.Linear(m, 952576),\n","                Reshape(-1, 16, 244, 244),\n","                nn.ConvTranspose2d(16, 16, stride=(1, 1), kernel_size=(4, 4), padding=0),\n","                nn.LeakyReLU(0.01),\n","                nn.ConvTranspose2d(16, 16, stride=(1, 1), kernel_size=(4, 4), padding=0),                \n","                nn.LeakyReLU(0.01),\n","                nn.ConvTranspose2d(16, 4, stride=(1, 1), kernel_size=(4, 4), padding=0),                \n","                nn.LeakyReLU(0.01),\n","                nn.ConvTranspose2d(4, 1, stride=(1, 1), kernel_size=(4, 4), padding=0), \n","                #Trim(),  # 1x29x29 -> 1x28x28\n","                #nn.Sigmoid()\n","                )\n","\n","    def forward(self, x):\n","        x = self.encoder(x)\n","        x = self.decoder(x)\n","        return x\n","\n","\n","\n","def run_cae(Rai, Raf, ti, tf, TRAIN_SIZE, BATCH_SIZE, SEED, RANDOM_SEED, LEARNING_RATE, NUM_EPOCHS, DEVICE, MIN_M, MAX_M, NUM_M):\n","\n","  data_x_diff, data_x_conv, data_y = load_data(Rai, Raf, ti, tf)\n","\n","  train_loader_diff, test_loader_diff, train_loader_conv, test_loader_conv = generate_loader(data_x_diff, data_x_conv, data_y, TRAIN_SIZE, BATCH_SIZE, SEED)\n","  \n","  #np.random.seed(0)\n","  #m_values_diff = np.random.randint(MIN_M, MAX_M, NUM_M)\n","\n","  m_values_diff = np.array([35, 37, 40, 42, 44, 45, 47, 55, 56, 57, 60, 62, 65, 67])\n","  m_values_conv = np.array([65, 63, 60, 58, 56, 55, 53, 45, 44, 43, 40, 38, 35, 33])\n","\n","\n","  print(m_values_diff)\n","  print(m_values_conv)\n","\n","  word = 'word'\n","\n","  i = 0\n","\n","  for md, mc in zip(m_values_diff, m_values_conv):\n","\n","    md = int(md)\n","    mc = int(mc)\n","\n","    print(md, mc)\n","    print()\n","\n","    if i != 0:\n","\n","      time.sleep(30)\n","\n","      model_diff = model_diff.cpu()\n","      model_conv = model_conv.cpu()\n","      del model_diff\n","      del model_conv\n","      del optimizer_diff\n","      del optimizer_conv\n","      del test_acc\n","      gc.collect()\n","      torch.cuda.empty_cache()\n","\n","      time.sleep(30)\n","\n","\n","    set_all_seeds(RANDOM_SEED)\n","\n","    model_diff = AutoEncoder(md)\n","    model_diff = model_diff.to(DEVICE)\n","\n","    model_conv = AutoEncoder(mc)\n","    model_conv = model_conv.to(DEVICE)\n","\n","    optimizer_diff = torch.optim.Adam(model_diff.parameters(), lr=LEARNING_RATE)\n","    optimizer_conv = torch.optim.Adam(model_conv.parameters(), lr=LEARNING_RATE)\n","  \n","    model_diff = model_diff.cpu()\n","    model_conv = model_conv.cpu()\n","    del model_diff\n","    del model_conv\n","    del optimizer_diff\n","    del optimizer_conv\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","\n","    model_diff = AutoEncoder(md)\n","    model_diff = model_diff.to(DEVICE)\n","\n","    model_conv = AutoEncoder(mc)\n","    model_conv = model_conv.to(DEVICE)\n","\n","    optimizer_diff = torch.optim.Adam(model_diff.parameters(), lr=LEARNING_RATE)\n","    optimizer_conv = torch.optim.Adam(model_conv.parameters(), lr=LEARNING_RATE)\n","\n","\n","    log_dict = train_autoencoder_v1(num_epochs=NUM_EPOCHS, model_diff=model_diff, model_conv=model_conv, \n","                                    optimizer_diff=optimizer_diff, optimizer_conv = optimizer_conv,\n","                                    train_loader_diff=train_loader_diff, train_loader_conv = train_loader_conv, train_loader_base = train_loader_conv, device = DEVICE,\n","                                    skip_epoch_stats=True)\n","  \n","    test_acc = compute_accuracy(model_diff, model_conv, test_loader_diff, test_loader_conv, test_loader_conv, DEVICE)\n","\n","\n","    word = 'diff_conv_casc'\n","\n","    os.mkdir('/content/gdrive/My Drive/Project/Results/' + Rai + '_' + Raf + '_' + str(ti) + '_' + str(tf)  + '/models/' + word + '/' + str(md) + '_' + str(mc) + '_RS' + str(RANDOM_SEED))\n","\n","    torch.save(model_diff.state_dict(), '/content/gdrive/My Drive/Project/Results/' + Rai + '_' + Raf + '_' + str(ti) + '_' + str(tf)  + '/models/' + word + '/' + str(md) + '_' + str(mc) + '_RS' + str(RANDOM_SEED) + '/model_diff.pt')\n","    torch.save(model_conv.state_dict(), '/content/gdrive/My Drive/Project/Results/' + Rai + '_' + Raf + '_' + str(ti) + '_' + str(tf)  + '/models/' + word + '/' + str(md) + '_' + str(mc) + '_RS' + str(RANDOM_SEED) + '/model_conv.pt')\n","\n","    np.save('/content/gdrive/My Drive/Project/Results/' + Rai + '_' + Raf + '_' + str(ti) + '_' + str(tf)  + '/models/' + word + '/' + str(md) + '_' + str(mc) + '_RS' + str(RANDOM_SEED) + '/test_acc.npy', test_acc, allow_pickle=True)\n","    np.save('/content/gdrive/My Drive/Project/Results/' + Rai + '_' + Raf + '_' + str(ti) + '_' + str(tf)  + '/models/' + word + '/' + str(md) + '_' + str(mc) + '_RS' + str(RANDOM_SEED) + '/log_dict.npy', log_dict, allow_pickle=True)\n","\n","    i += 1\n","\n","    print()\n","  "]}]}