{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PILS-MLP.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPCuOXO3+YyG4ZTccKUkDyj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"hvaDgoXUGiy9"},"outputs":[],"source":["from google.colab import drive\n","drive.mount(\"/content/gdrive\")\n","\n","import time\n","import os\n","import glob\n","import random\n","import json\n","import subprocess\n","import sys\n","import gc\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from collections import OrderedDict\n","import xml.etree.ElementTree\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","from sklearn.decomposition import PCA\n","def swap(i,j):\n","  a = i.swapaxes(0,1)\n","  b = j.swapaxes(0,1)\n","  return a,b\n","\n","def unsqueeze(i,j):\n","  a = torch.unsqueeze(i,0)\n","  b = torch.unsqueeze(j,0)\n","  return a,b\n","def set_deterministic():\n","    if torch.cuda.is_available():\n","        torch.backends.cudnn.benchmark = False\n","        torch.backends.cudnn.deterministic = True\n","    torch.set_deterministic(True)\n","    \n","    \n","def set_all_seeds(seed):\n","    os.environ[\"PL_GLOBAL_SEED\"] = str(seed)\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","def compute_test_accuracy(Rai, Raf, ti, tf, mode, TRAIN_SIZE, BATCH_SIZE, SEED, var, model, device):\n","\n","  data_x, data_y = load_data(Rai, Raf, ti, tf, mode)\n","\n","  image_train_loader, image_test_loader = generate_image_loader(data_x, data_y, TRAIN_SIZE, BATCH_SIZE, SEED)  \n","\n","  data_x, data_y, n_components_x, n_components_y, pca_x, pca_y = principal_components(data_x, data_y, var)\n","\n","  train_loader, test_loader = generate_loader(data_x, data_y, TRAIN_SIZE, BATCH_SIZE, SEED)\n","\n","  model.eval()\n","  numers = []\n","  denoms = []\n","\n","  refs_list = []\n","\n","  with torch.no_grad():\n","  \n","    for i, (_, refs) in enumerate(image_test_loader):\n","\n","      refs = refs.to(device)\n","\n","      refs = np.array(refs.cpu())\n","\n","      refs = np.reshape(refs, (np.shape(refs)[0], -1))\n","\n","      refs_list = refs_list + [refs]\n","\n","\n","  with torch.no_grad():\n","  \n","    for i, (features, targets) in enumerate(test_loader):\n","\n","      print(np.shape(features))\n","\n","      features = features.to(device)\n","\n","      pred = model(features)\n","\n","      pred = np.array(pred.cpu())\n","      references = refs_list[i]\n","\n","      pred = pca_y.inverse_transform(pred)\n","\n","      numer = np.sum(np.square(np.array(references) - np.array(pred)))\n","      numers = numers + [numer]\n","\n","      denom = np.sum(np.square(np.array(references)))\n","      denoms = denoms + [denom]\n","\n","\n","  acc = 1 - np.array(numers)/np.array(denoms)\n","\n","  return np.round(acc[0], 5)\n","\n","def compute_test_accuracy2(Rai, Raf, ti, tf, TRAIN_SIZE, BATCH_SIZE, SEED, var, model_diff, model_conv, device):\n","\n","  data_x, data_y = load_data(Rai, Raf, ti, tf, mode = 'no_masking')\n","  image_train_loader, image_test_loader = generate_image_loader(data_x, data_y, TRAIN_SIZE, BATCH_SIZE, SEED)  \n","\n","  data_x_diff, data_y_diff = load_data(Rai, Raf, ti, tf, mode = 'diffusion_mask')\n","  data_x_diff, data_y_diff, n_components_x_diff, n_components_y_diff, pca_x_diff, pca_y_diff = principal_components(data_x_diff, data_y_diff, var)\n","  train_loader_diff, test_loader_diff = generate_loader(data_x_diff, data_y_diff, TRAIN_SIZE, BATCH_SIZE, SEED)\n","\n","  data_x_conv, data_y_conv = load_data(Rai, Raf, ti, tf, mode = 'convection_mask')\n","  data_x_conv, data_y_conv, n_components_x_conv, n_components_y_conv, pca_x_conv, pca_y_conv = principal_components(data_x_conv, data_y_conv, var)\n","  train_loader_conv, test_loader_conv = generate_loader(data_x_conv, data_y_conv, TRAIN_SIZE, BATCH_SIZE, SEED)\n","\n","\n","  numers = []\n","  denoms = []\n","\n","  refs_list = []\n","\n","  with torch.no_grad():\n","  \n","    for i, (_, refs) in enumerate(image_test_loader):\n","\n","      print(i)\n","\n","      refs = refs.to(device)\n","\n","      refs = np.array(refs.cpu())\n","\n","      refs = np.reshape(refs, (np.shape(refs)[0], -1))\n","\n","      print(np.shape(refs))\n","\n","      refs_list = refs_list + [refs]\n","\n","  #print(np.shape(refs_list[0]),np.shape(refs_list[1]),np.shape(refs_list[2]))\n","\n","\n","  with torch.no_grad():\n","\n","    p = 0\n","\n","    for (features_diff, _), (features_conv, _) in zip(test_loader_diff, test_loader_conv):\n","\n","      #print(np.shape(refs_list[0]),np.shape(refs_list[1]),np.shape(refs_list[2]))\n","\n","      features_diff = features_diff.to(device)\n","      features_conv = features_conv.to(device)\n","\n","      print(np.shape(features_diff))\n","      print(np.shape(features_conv))\n","\n","      pred_diff = model_diff(features_diff) \n","      pred_conv = model_conv(features_conv)\n","\n","      print(np.shape(pred_diff))\n","      print(np.shape(pred_conv))\n","\n","      pred_diff = np.array(pred_diff.cpu())\n","      pred_conv = np.array(pred_conv.cpu())\n","\n","      print(np.shape(pred_diff))\n","      print(np.shape(pred_conv))\n","\n","      #references = refs_list[p]\n","\n","      print(np.shape(refs_list[p]))\n","\n","      pred_diff = pca_y_diff.inverse_transform(pred_diff)\n","\n","      pred_conv = pca_y_conv.inverse_transform(pred_conv)\n","\n","      pred = pred_diff + pred_conv\n","\n","      print(np.shape(pred))\n","\n","      numer = np.sum(np.square(np.array(refs_list[p]) - np.array(pred)))\n","      numers = numers + [numer]\n","\n","      denom = np.sum(np.square(np.array(refs_list[p])))\n","      denoms = denoms + [denom]\n","\n","      p = p + 1\n","\n","  acc = 1 - np.array(numers)/np.array(denoms)\n","\n","  return np.round(acc[0], 5)\n","\n","\n","def compute_epoch_loss_autoencoder(model, data_loader, loss_fn, device):\n","  model.eval()\n","  curr_loss, num_examples = 0., 0\n","  with torch.no_grad():\n","    for features, targets in data_loader:\n","\n","      features = features.to(device)\n","      targets = targets.to(device)\n","\n","      predictions = model(features)\n","      loss = loss_fn(predictions, targets, reduction='sum')\n","      num_examples += targets.size(0)\n","      curr_loss += loss\n","\n","      features = features.to('cpu')\n","      targets = targets.to('cpu')\n","\n","      del features\n","      del targets\n","\n","  curr_loss = curr_loss / num_examples\n","\n","  return curr_loss\n","def train_autoencoder_v1(num_epochs, model, optimizer, \n","                         train_loader, device, loss_fn=None, \n","                         skip_epoch_stats=False,\n","                         save_model=None):\n","    \n","    log_dict = {'train_loss_per_batch': [],\n","                'train_loss_per_epoch': []}\n","    \n","    if loss_fn is None:\n","        loss_fn = F.mse_loss\n","\n","    start_time = time.time()\n","    for epoch in range(num_epochs):\n","\n","        model.train()\n","        for batch_idx, (features, targets) in enumerate(train_loader):\n","\n","            features = features.to(device)\n","            targets = targets.to(device)\n","\n","            # FORWARD AND BACK PROP\n","            predictions = model(features)\n","            loss = loss_fn(predictions, targets)\n","            optimizer.zero_grad()\n","\n","            #loss.requires_grad = True\n","\n","            loss.backward()\n","\n","            # UPDATE MODEL PARAMETERS\n","            optimizer.step()\n","\n","            #additional\n","\n","            features = features.to('cpu')\n","            targets = targets.to('cpu')\n","\n","            del features\n","            del targets\n","\n","            # LOGGING\n","            log_dict['train_loss_per_batch'].append(loss.item())\n","            \n","            print('Epoch: %03d/%03d | Batch %04d/%04d | Loss: %.4f'\n","                  % (epoch+1, num_epochs, batch_idx+1,\n","                       len(train_loader), loss))\n","\n","        if not skip_epoch_stats:\n","            model.eval()\n","            \n","            with torch.set_grad_enabled(False):  # save memory during inference\n","                \n","                train_loss = compute_epoch_loss_autoencoder(\n","                    model, train_loader, loss_fn, device)\n","                print('***Epoch: %03d/%03d | Loss: %.3f' % (\n","                      epoch+1, num_epochs, train_loss))\n","                log_dict['train_loss_per_epoch'].append(train_loss.item())\n","\n","        print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n","\n","    print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))\n","    if save_model is not None:\n","        torch.save(model.state_dict(), save_model)\n","    \n","    return log_dict\n","def plot_training_loss(minibatch_losses, num_epochs, averaging_iterations=100, custom_label=''):\n","\n","    iter_per_epoch = len(minibatch_losses) // num_epochs\n","\n","    plt.figure()\n","    ax1 = plt.subplot(1, 1, 1)\n","    ax1.plot(range(len(minibatch_losses)),\n","             (minibatch_losses), label=f'Minibatch Loss{custom_label}')\n","    ax1.set_xlabel('Iterations')\n","    ax1.set_ylabel('Loss')\n","\n","    if len(minibatch_losses) < 1000:\n","        num_losses = len(minibatch_losses) // 2\n","    else:\n","      num_losses = 1000\n","\n","    #ax1.set_ylim([\n","    #    0, np.max(minibatch_losses[num_losses:])*1.5\n","    #    ])\n","\n","    ax1.plot(np.convolve(minibatch_losses,\n","                         np.ones(averaging_iterations,)/averaging_iterations,\n","                         mode='valid'),\n","             label=f'Running Average{custom_label}')\n","    ax1.legend()\n","\n","def plot_accuracy(train_acc, valid_acc):\n","\n","    num_epochs = len(train_acc)\n","\n","    plt.plot(np.arange(1, num_epochs+1), \n","             train_acc, label='Training')\n","    plt.plot(np.arange(1, num_epochs+1),\n","             valid_acc, label='Validation')\n","\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Accuracy')\n","    plt.legend()\n","def load_data(Rai, Raf, ti, tf, mode):\n","\n","  file_count = 0\n","\n","  if mode == 'no_masking':\n","\n","    _, _, files = next(os.walk('/content/gdrive/My Drive/Project/Results/' + Rai + '_' + Raf + '_13_1_19'  + '/snapshots/' + str(ti) + '/arrays'))\n","    file_count = len(files)\n","\n","  elif mode == 'diffusion_mask':\n","\n","    _, _, files = next(os.walk('/content/gdrive/My Drive/Project/Results/' + Rai + '_' + Raf + '_13_1_19'  + '/masks/diffusion/' + str(ti) + '/arrays'))\n","    file_count = len(files)\n","\n","  elif mode == 'convection_mask':\n","\n","    _, _, files = next(os.walk('/content/gdrive/My Drive/Project/Results/' + Rai + '_' + Raf + '_13_1_19'  + '/masks/convection/' + str(ti) + '/arrays'))\n","    file_count = len(files)\n","\n","\n","  data_x = []\n","  file_list_x = []\n","\n","  data_y = []\n","  file_list_y = []\n","\n","\n","  for i in range(file_count):\n","\n","    if i > 2140: \n","      continue  \n","\n","    if mode == 'no_masking':\n","\n","      file_e = glob.glob('/content/gdrive/My Drive/Project/Results/' + Rai + '_' + Raf + '_' + '13_1_19' + '/snapshots/' + str(ti) + '/arrays/' + str(i) + '_' + '*.npy')\n","      file_list_x = file_list_x + [file_e]\n","\n","    elif mode == 'diffusion_mask':\n","      file_e = glob.glob('/content/gdrive/My Drive/Project/Results/' + Rai + '_' + Raf + '_' + '13_1_19' + '/masks/diffusion/' + str(ti) + '/arrays/' + str(i) + '_' + '*.npy')\n","      file_list_x = file_list_x + [file_e]\n","\n","    elif mode == 'convection_mask':\n","      file_e = glob.glob('/content/gdrive/My Drive/Project/Results/' + Rai + '_' + Raf + '_' + '13_1_19' + '/masks/convection/' + str(ti) + '/arrays/' + str(i) + '_' + '*.npy')\n","      file_list_x = file_list_x + [file_e]\n","\n","  file_list_x = np.ravel(file_list_x)\n","\n","  for file_path in file_list_x:\n","    \n","    a = np.load(file_path, allow_pickle=True)\n","\n","    data_x = data_x + [a]\n","\n","  for i in range(file_count):\n","\n","    if i > 2140: \n","      continue \n","\n","    if mode == 'no_masking':\n","\n","      file_e = glob.glob('/content/gdrive/My Drive/Project/Results/' + Rai + '_' + Raf + '_' + '13_1_19'  + '/snapshots/' + str(tf) + '/arrays/' + str(i) + '_' + '*.npy')\n","      file_list_y = file_list_y + [file_e]\n","\n","    elif mode == 'diffusion_mask':\n","\n","      file_e = glob.glob('/content/gdrive/My Drive/Project/Results/' + Rai + '_' + Raf + '_' + '13_1_19'  + '/masks/diffusion/' + str(tf) + '/arrays/' + str(i) + '_' + '*.npy')\n","      file_list_y = file_list_y + [file_e]\n","  \n","    elif mode == 'convection_mask':\n","\n","      file_e = glob.glob('/content/gdrive/My Drive/Project/Results/' + Rai + '_' + Raf + '_' + '13_1_19'  + '/masks/convection/' + str(tf) + '/arrays/' + str(i) + '_' + '*.npy')\n","      file_list_y = file_list_y + [file_e]\n","\n","  file_list_y = np.ravel(file_list_y)\n","\n","  for file_path in file_list_y:\n","    \n","    a = np.load(file_path, allow_pickle=True)\n","\n","    data_y = data_y + [a]\n","  \n","  return data_x, data_y\n","def print_files(Rai, Raf, ti, tf, mode):\n","\n","  file_count = 0\n","\n","  if mode == 'no_masking':\n","\n","    _, _, files = next(os.walk('/content/gdrive/My Drive/Project/Results/' + Rai + '_' + Raf + '_13_1_19'  + '/snapshots/' + str(ti) + '/arrays'))\n","    file_count = len(files)\n","\n","  elif mode == 'diffusion_mask':\n","\n","    _, _, files = next(os.walk('/content/gdrive/My Drive/Project/Results/' + Rai + '_' + Raf + '_13_1_19'  + '/masks/diffusion/' + str(ti) + '/arrays'))\n","    file_count = len(files)\n","\n","  elif mode == 'convection_mask':\n","\n","    _, _, files = next(os.walk('/content/gdrive/My Drive/Project/Results/' + Rai + '_' + Raf + '_13_1_19'  + '/masks/convection/' + str(ti) + '/arrays'))\n","    file_count = len(files)\n","\n","\n","  data_x = []\n","  file_list_x = []\n","\n","  data_y = []\n","  file_list_y = []\n","\n","\n","  for i in range(file_count):\n","\n","    if i > 2140: \n","      continue  \n","\n","    if mode == 'no_masking':\n","\n","      file_e = glob.glob('/content/gdrive/My Drive/Project/Results/' + Rai + '_' + Raf + '_' + '13_1_19' + '/snapshots/' + str(ti) + '/arrays/' + str(i) + '_' + '*.npy')\n","      file_list_x = file_list_x + [file_e]\n","\n","    elif mode == 'diffusion_mask':\n","      file_e = glob.glob('/content/gdrive/My Drive/Project/Results/' + Rai + '_' + Raf + '_' + '13_1_19' + '/masks/diffusion/' + str(ti) + '/arrays/' + str(i) + '_' + '*.npy')\n","      file_list_x = file_list_x + [file_e]\n","\n","    elif mode == 'convection_mask':\n","      file_e = glob.glob('/content/gdrive/My Drive/Project/Results/' + Rai + '_' + Raf + '_' + '13_1_19' + '/masks/convection/' + str(ti) + '/arrays/' + str(i) + '_' + '*.npy')\n","      file_list_x = file_list_x + [file_e]\n","\n","  file_list_x = np.ravel(file_list_x)\n","\n","\n","  for i in range(file_count):\n","\n","    if i > 2140: \n","      continue \n","\n","    if mode == 'no_masking':\n","\n","      file_e = glob.glob('/content/gdrive/My Drive/Project/Results/' + Rai + '_' + Raf + '_' + '13_1_19'  + '/snapshots/' + str(tf) + '/arrays/' + str(i) + '_' + '*.npy')\n","      file_list_y = file_list_y + [file_e]\n","\n","    elif mode == 'diffusion_mask':\n","\n","      file_e = glob.glob('/content/gdrive/My Drive/Project/Results/' + Rai + '_' + Raf + '_' + '13_1_19'  + '/masks/diffusion/' + str(tf) + '/arrays/' + str(i) + '_' + '*.npy')\n","      file_list_y = file_list_y + [file_e]\n","  \n","    elif mode == 'convection_mask':\n","\n","      file_e = glob.glob('/content/gdrive/My Drive/Project/Results/' + Rai + '_' + Raf + '_' + '13_1_19'  + '/masks/convection/' + str(tf) + '/arrays/' + str(i) + '_' + '*.npy')\n","      file_list_y = file_list_y + [file_e]\n","\n","  file_list_y = np.ravel(file_list_y)\n","\n","  \n","  return file_list_x, file_list_y\n","def split_files(file_list_x, file_list_y, train_size, seed):\n","\n","\n","  file_list_x = random.Random(seed).sample(list(file_list_x), len(file_list_x))\n","  file_list_y = random.Random(seed).sample(list(file_list_y), len(file_list_y))\n","\n","\n","  middle_index = int(np.round(len(file_list_x) * train_size))\n","\n","  file_list_x_train = file_list_x[:middle_index]\n","  file_list_x_test = file_list_x[middle_index:]\n","\n","  file_list_y_train = file_list_y[:middle_index]\n","  file_list_y_test = file_list_y[middle_index:]\n","\n","  return file_list_x_train, file_list_y_train, file_list_x_test, file_list_y_test\n","def principal_components(data_x, data_y, var = 0.95):\n","\n","  data_x = np.reshape(data_x, (np.shape(data_x)[0], -1))\n","  data_y = np.reshape(data_y, (np.shape(data_y)[0], -1))\n","\n","  pca_x = PCA(var)\n","  pca_y = PCA(var)\n","\n","  data_x = pca_x.fit_transform(data_x)\n","  n_components_x = pca_x.n_components_\n","\n","  data_y = pca_y.fit_transform(data_y)\n","  n_components_y = pca_y.n_components_\n","\n","\n","  return data_x.tolist(), data_y.tolist(), n_components_x, n_components_y, pca_x, pca_y\n","def split_x_y_list(data_x, data_y, train_size, seed):\n","\n","\n","  data_x = random.Random(seed).sample(data_x, len(data_x))\n","  data_y = random.Random(seed).sample(data_y, len(data_y))\n","\n","\n","  middle_index = int(np.round(len(data_x) * train_size))\n","\n","  data_x_train = data_x[:middle_index]\n","  data_x_test = data_x[middle_index:]\n","\n","  data_y_train = data_y[:middle_index]\n","  data_y_test = data_y[middle_index:]\n","\n","  return data_x_train, data_y_train, data_x_test, data_y_test\n","def generate_loader(data_x, data_y, TRAIN_SIZE, BATCH_SIZE, SEED):\n","\n","  data_x_train, data_y_train, data_x_test, data_y_test = split_x_y_list(data_x, data_y, TRAIN_SIZE, SEED)\n","\n","  ##########################\n","  ### Dataset\n","  ##########################\n","\n","  import torch\n","  import numpy as np\n","  from torch.utils.data import TensorDataset, DataLoader\n","\n","  import random\n","\n","  my_x_train = np.array(data_x_train)\n","  my_x_test = np.array(data_x_test)\n","\n","  my_y_train = np.array(data_y_train) \n","  my_y_test = np.array(data_y_test)\n","\n","  tensor_x_train = torch.Tensor(my_x_train) # transform to torch tensor\n","  tensor_x_test = torch.Tensor(my_x_test)\n","  tensor_y_train = torch.Tensor(my_y_train) \n","  tensor_y_test = torch.Tensor(my_y_test)\n","\n","  #tensor_x_train, tensor_x_test, tensor_y_train, tensor_y_test = tensor_x_train.to(DEVICE), tensor_x_test.to(DEVICE), tensor_y_train.to(DEVICE), tensor_y_test.to(DEVICE)\n","\n","  train_dataset = TensorDataset(tensor_x_train, tensor_y_train)\n","  test_dataset = TensorDataset(tensor_x_test, tensor_y_test)  \n","\n","  train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE) \n","  test_loader = DataLoader(test_dataset, batch_size = BATCH_SIZE) \n","\n","\n","  #train_loader = [unsqueeze(i,j) for (i,j) in train_loader]\n","  #train_loader = [swap(i,j) for (i,j) in train_loader]\n","\n","  #test_loader = [unsqueeze(i,j) for (i,j) in test_loader]\n","  #test_loader = [swap(i,j) for (i,j) in test_loader]\n","\n","  # Checking the dataset\n","  print('Training Set:\\n')\n","  for image_x, image_y in train_loader: \n","      print('Image x batch dimensions:', image_x.size())\n","      print('Image y batch dimensions:', image_y.size())\n","  \n","\n","  # Checking the dataset\n","  print('\\nTesting Set:')\n","  for image_x, image_y in test_loader:\n","      print('Image x batch dimensions:', image_x.size())\n","      print('Image y batch dimensions:', image_y.size())\n"," \n","\n","  return train_loader, test_loader\n","\n","def generate_image_loader(data_x, data_y, TRAIN_SIZE, BATCH_SIZE, SEED):\n","\n","  data_x_train, data_y_train, data_x_test, data_y_test = split_x_y_list(data_x, data_y, TRAIN_SIZE, SEED)\n","\n","  ##########################\n","  ### Dataset\n","  ##########################\n","\n","  import torch\n","  import numpy as np\n","  from torch.utils.data import TensorDataset, DataLoader\n","\n","  import random\n","\n","  my_x_train = np.array(data_x_train)\n","  my_x_test = np.array(data_x_test)\n","\n","  my_y_train = np.array(data_y_train) \n","  my_y_test = np.array(data_y_test)\n","\n","  tensor_x_train = torch.Tensor(my_x_train) # transform to torch tensor\n","  tensor_x_test = torch.Tensor(my_x_test)\n","  tensor_y_train = torch.Tensor(my_y_train) \n","  tensor_y_test = torch.Tensor(my_y_test)\n","\n","  #tensor_x_train, tensor_x_test, tensor_y_train, tensor_y_test = tensor_x_train.to(DEVICE), tensor_x_test.to(DEVICE), tensor_y_train.to(DEVICE), tensor_y_test.to(DEVICE)\n","\n","  train_dataset = TensorDataset(tensor_x_train, tensor_y_train)\n","  test_dataset = TensorDataset(tensor_x_test, tensor_y_test)  \n","\n","  train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE) \n","  test_loader = DataLoader(test_dataset, batch_size = BATCH_SIZE) \n","\n","\n","  train_loader = [unsqueeze(i,j) for (i,j) in train_loader]\n","  train_loader = [swap(i,j) for (i,j) in train_loader]\n","\n","  test_loader = [unsqueeze(i,j) for (i,j) in test_loader]\n","  test_loader = [swap(i,j) for (i,j) in test_loader]\n","\n","  # Checking the dataset\n","  print('Training Set:\\n')\n","  for image_x, image_y in train_loader: \n","      print('Image x batch dimensions:', image_x.size())\n","      print('Image y batch dimensions:', image_y.size())\n","     \n","\n","  # Checking the dataset\n","  print('\\nTesting Set:')\n","  for image_x, image_y in test_loader:\n","      print('Image x batch dimensions:', image_x.size())\n","      print('Image y batch dimensions:', image_y.size())\n"," \n","\n","  return train_loader, test_loader\n","\n","##########################\n","### MODEL\n","##########################\n","\n","\n","class Reshape(nn.Module):\n","    def __init__(self, *args):\n","        super().__init__()\n","        self.shape = args\n","\n","    def forward(self, x):\n","        return x.view(self.shape)\n","\n","\n","class Trim(nn.Module):\n","    def __init__(self, *args):\n","        super().__init__()\n","\n","    def forward(self, x):\n","        return x[:, :, :256, :256]\n","\n","\n","class MLP(nn.Module):\n","\n","    def __init__(self, n_components_x, n_components_y):\n","        super().__init__()\n","        \n","        self.ANN = nn.Sequential( #784\n","                nn.Linear(n_components_x, 100),\n","                nn.LeakyReLU(0.01),\n","                nn.Linear(100, 100),\n","                nn.LeakyReLU(0.01),\n","                nn.Linear(100, 100),\n","                nn.LeakyReLU(0.01),\n","                nn.Linear(100, 100),\n","                nn.LeakyReLU(0.01),\n","                nn.Linear(100, n_components_y)\n","                )\n","\n","    def forward(self, x):\n","        x = self.ANN(x)\n","        return x\n","def run_cae(Rai, Raf, ti, tf, mode, pca_var, TRAIN_SIZE, BATCH_SIZE, SEED, RANDOM_SEED, LEARNING_RATE, NUM_EPOCHS, DEVICE):\n","\n","  data_x, data_y = load_data(Rai, Raf, ti, tf, mode)\n","\n","  data_y_ref = data_y.copy()\n","\n","  data_x, data_y, n_components_x, n_components_y, pca_x, pca_y = principal_components(data_x, data_y, pca_var)\n","\n","  train_loader, test_loader = generate_loader(data_x, data_y, TRAIN_SIZE, BATCH_SIZE, SEED)  \n","\n","  word = 'word'\n","  \n","  print(n_components_x, n_components_y)\n","\n","\n","  #time.sleep(30)\n","  model = MLP(n_components_x, n_components_y)\n","  model = model.cpu()\n","  optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","  #test_acc = 0\n","  del model\n","  del optimizer\n","  #del test_acc\n","  gc.collect()\n","  torch.cuda.empty_cache()\n","\n","  #time.sleep(30)\n","\n","\n","  set_all_seeds(RANDOM_SEED)\n","\n","  model = MLP(n_components_x, n_components_y)\n","  model = model.to(DEVICE)\n","\n","  optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","  \n","  model = model.cpu()\n","  del model\n","  del optimizer\n","  gc.collect()\n","  torch.cuda.empty_cache()\n","\n","  model = MLP(n_components_x, n_components_y)\n","  model = model.to(DEVICE)\n","\n","  optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","\n","\n","  log_dict = train_autoencoder_v1(num_epochs=NUM_EPOCHS, model=model, \n","                                  optimizer=optimizer,\n","                                  train_loader=train_loader, device = DEVICE,\n","                                  skip_epoch_stats=True)\n","  \n","  test_acc = compute_test_accuracy(Rai, Raf, ti, tf, mode, TRAIN_SIZE, BATCH_SIZE, SEED, pca_var, model, DEVICE)\n","\n","  print(test_acc)\n","\n","  if mode == 'diffusion_mask':\n","    word = 'diffusion'\n","  elif mode == 'convection_mask':\n","    word = 'convection'\n","  elif mode == 'no_masking':\n","    word = 'base'\n","\n","  os.mkdir('/content/gdrive/My Drive/Project/Results/' + Rai + '_' + Raf + '_' + str(ti) + '_' + str(tf)  + '/models/' + word + '/' + 'PCA_ANN_' + str(n_components_x) + '_' + str(n_components_y) + '_' + str(pca_var) + '_RS' + str(RANDOM_SEED))\n","\n","  torch.save(model.state_dict(), '/content/gdrive/My Drive/Project/Results/' + Rai + '_' + Raf + '_' + str(ti) + '_' + str(tf)  + '/models/' + word + '/' + 'PCA_ANN_' + str(n_components_x) + '_' + str(n_components_y) + '_' + str(pca_var) + '_RS' + str(RANDOM_SEED) + '/model.pt')\n","\n","  np.save('/content/gdrive/My Drive/Project/Results/' + Rai + '_' + Raf + '_' + str(ti) + '_' + str(tf)  + '/models/' + word + '/' + 'PCA_ANN_' + str(n_components_x) + '_' + str(n_components_y) + '_' + str(pca_var) + '_RS' + str(RANDOM_SEED) + '/test_acc.npy', test_acc, allow_pickle=True)\n","  np.save('/content/gdrive/My Drive/Project/Results/' + Rai + '_' + Raf + '_' + str(ti) + '_' + str(tf)  + '/models/' + word + '/' + 'PCA_ANN_' + str(n_components_x) + '_' + str(n_components_y) + '_' + str(pca_var) + '_RS' + str(RANDOM_SEED) + '/log_dict.npy', log_dict, allow_pickle=True)\n","\n","  print()\n","  "]}]}