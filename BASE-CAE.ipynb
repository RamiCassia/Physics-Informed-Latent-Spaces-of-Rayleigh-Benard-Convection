{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CAE1.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNJamV5dUchGXodYxgWRTgt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"hvaDgoXUGiy9"},"outputs":[],"source":["from google.colab import drive\n","drive.mount(\"/content/gdrive\")\n","\n","import time\n","import os\n","import glob\n","import random\n","import json\n","import subprocess\n","import sys\n","import gc\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from collections import OrderedDict\n","import xml.etree.ElementTree\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import TensorDataset, DataLoader\n","def set_deterministic():\n","    if torch.cuda.is_available():\n","        torch.backends.cudnn.benchmark = False\n","        torch.backends.cudnn.deterministic = True\n","    torch.set_deterministic(True)\n","    \n","    \n","def set_all_seeds(seed):\n","    os.environ[\"PL_GLOBAL_SEED\"] = str(seed)\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","def compute_accuracy(model, data_loader, device):\n","    model.eval()\n","    numers = []\n","    denoms = []\n","    with torch.no_grad():\n","  \n","        for i, (features, targets) in enumerate(data_loader):\n","\n","            targets = targets.to(device)\n","\n","            features = features.to(device)\n","\n","            recon = model(features)\n","\n","            numer = np.sum(np.square(np.array(targets.cpu()) - np.array(recon.cpu())))\n","            numers = numers + [numer]\n","\n","            denom = np.sum(np.square(np.array(targets.cpu())))\n","            denoms = denoms + [denom]\n","\n","    acc = (1 - (np.array(np.ravel(numers))/np.array(np.ravel(denoms))))\n","\n","    return np.round(acc[0], 5)\n","    \n","\n","def compute_epoch_loss_autoencoder(model, data_loader, loss_fn, device):\n","    model.eval()\n","    curr_loss, num_examples = 0., 0\n","    with torch.no_grad():\n","        for features, targets in data_loader:\n","\n","            features = features.to(device)\n","            targets = targets.to(device)\n","\n","            predictions = model(features)\n","            loss = loss_fn(predictions, targets, reduction='sum')\n","            num_examples += targets.size(0)\n","            curr_loss += loss\n","\n","            features = features.to('cpu')\n","            targets = targets.to('cpu')\n","\n","            del features\n","            del targets\n","\n","        curr_loss = curr_loss / num_examples\n","        return curr_loss\n","def train_autoencoder_v1(num_epochs, model, optimizer, \n","                         train_loader, device, loss_fn=None, \n","                         skip_epoch_stats=False,\n","                         save_model=None):\n","    \n","    log_dict = {'train_loss_per_batch': [],\n","                'train_loss_per_epoch': []}\n","    \n","    if loss_fn is None:\n","        loss_fn = F.mse_loss\n","\n","    start_time = time.time()\n","    for epoch in range(num_epochs):\n","\n","        model.train()\n","        for batch_idx, (features, targets) in enumerate(train_loader):\n","\n","            features = features.to(device)\n","            targets = targets.to(device)\n","\n","            # FORWARD AND BACK PROP\n","            predictions = model(features)\n","            loss = loss_fn(predictions, targets)\n","            optimizer.zero_grad()\n","\n","            loss.backward()\n","\n","            # UPDATE MODEL PARAMETERS\n","            optimizer.step()\n","\n","            #additional\n","\n","            features = features.to('cpu')\n","            targets = targets.to('cpu')\n","\n","            del features\n","            del targets\n","\n","            # LOGGING\n","            log_dict['train_loss_per_batch'].append(loss.item())\n","            \n","            print('Epoch: %03d/%03d | Batch %04d/%04d | Loss: %.4f'\n","                  % (epoch+1, num_epochs, batch_idx+1,\n","                       len(train_loader), loss))\n","\n","        if not skip_epoch_stats:\n","            model.eval()\n","            \n","            with torch.set_grad_enabled(False):  # save memory during inference\n","                \n","                train_loss = compute_epoch_loss_autoencoder(\n","                    model, train_loader, loss_fn, device)\n","                print('***Epoch: %03d/%03d | Loss: %.3f' % (\n","                      epoch+1, num_epochs, train_loss))\n","                log_dict['train_loss_per_epoch'].append(train_loss.item())\n","\n","        print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n","\n","    print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))\n","    if save_model is not None:\n","        torch.save(model.state_dict(), save_model)\n","    \n","    return log_dict\n","def plot_training_loss(minibatch_losses, num_epochs, averaging_iterations=100, custom_label=''):\n","\n","    iter_per_epoch = len(minibatch_losses) // num_epochs\n","\n","    plt.figure()\n","    ax1 = plt.subplot(1, 1, 1)\n","    ax1.plot(range(len(minibatch_losses)),\n","             (minibatch_losses), label=f'Minibatch Loss{custom_label}')\n","    ax1.set_xlabel('Iterations')\n","    ax1.set_ylabel('Loss')\n","\n","    if len(minibatch_losses) < 1000:\n","        num_losses = len(minibatch_losses) // 2\n","    else:\n","      num_losses = 1000\n","\n","    #ax1.set_ylim([\n","    #    0, np.max(minibatch_losses[num_losses:])*1.5\n","    #    ])\n","\n","    ax1.plot(np.convolve(minibatch_losses,\n","                         np.ones(averaging_iterations,)/averaging_iterations,\n","                         mode='valid'),\n","             label=f'Running Average{custom_label}')\n","    ax1.legend()\n","\n","def plot_accuracy(train_acc, valid_acc):\n","\n","    num_epochs = len(train_acc)\n","\n","    plt.plot(np.arange(1, num_epochs+1), \n","             train_acc, label='Training')\n","    plt.plot(np.arange(1, num_epochs+1),\n","             valid_acc, label='Validation')\n","\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Accuracy')\n","    plt.legend()\n","def load_data(Rai, Raf, ti, tf, mode):\n","\n","  file_count = 0\n","\n","  if mode == 'no_masking':\n","\n","    _, _, files = next(os.walk('/content/gdrive/My Drive/Project/Results/' + Rai + '_' + Raf + '_13_1_19' + '/snapshots/' + str(ti) + '/arrays'))\n","    file_count = len(files)\n","\n","  elif mode == 'diffusion_mask':\n","\n","    _, _, files = next(os.walk('/content/gdrive/My Drive/Project/Results/' + Rai + '_' + Raf + '_13_1_19' + '/masks/diffusion/' + str(ti) + '/arrays'))\n","    file_count = len(files)\n","\n","  elif mode == 'convection_mask':\n","\n","    _, _, files = next(os.walk('/content/gdrive/My Drive/Project/Results/' + Rai + '_' + Raf + '_13_1_19' + '/masks/convection/' + str(ti) + '/arrays'))\n","    file_count = len(files)\n","\n","\n","  print(file_count)\n","  data_x = []\n","  file_list_x = []\n","\n","  data_y = []\n","  file_list_y = []\n","\n","  for i in range(file_count):\n","\n","    if i > 2140: \n","      continue  \n","\n","    if mode == 'no_masking':\n","\n","      file_e = glob.glob('/content/gdrive/My Drive/Project/Results/' + Rai + '_' + Raf + '_13_1_19' + '/snapshots/' + str(ti) + '/arrays/' + str(i) + '_' + '*.npy')\n","      file_list_x = file_list_x + [file_e]\n","\n","    elif mode == 'diffusion_mask':\n","      file_e = glob.glob('/content/gdrive/My Drive/Project/Results/' + Rai + '_' + Raf + '_13_1_19' + '/masks/diffusion/' + str(ti) + '/arrays/' + str(i) + '_' + '*.npy')\n","      file_list_x = file_list_x + [file_e]\n","\n","    elif mode == 'convection_mask':\n","      file_e = glob.glob('/content/gdrive/My Drive/Project/Results/' + Rai + '_' + Raf + '_13_1_19' + '/masks/convection/' + str(ti) + '/arrays/' + str(i) + '_' + '*.npy')\n","      file_list_x = file_list_x + [file_e]\n","\n","  file_list_x = np.ravel(file_list_x)\n","\n","  for file_path in file_list_x:\n","    \n","    a = np.load(file_path, allow_pickle=True)\n","\n","    data_x = data_x + [a]\n","\n","  for i in range(file_count):\n","\n","    if i > 2140: \n","      continue  \n","\n","    if mode == 'no_masking':\n","\n","      file_e = glob.glob('/content/gdrive/My Drive/Project/Results/' + Rai + '_' + Raf + '_13_1_19' + '/snapshots/' + str(tf) + '/arrays/' + str(i) + '_' + '*.npy')\n","      file_list_y = file_list_y + [file_e]\n","\n","    elif mode == 'diffusion_mask':\n","\n","      file_e = glob.glob('/content/gdrive/My Drive/Project/Results/' + Rai + '_' + Raf + '_13_1_19' + '/masks/diffusion/' + str(tf) + '/arrays/' + str(i) + '_' + '*.npy')\n","      file_list_y = file_list_y + [file_e]\n","  \n","    elif mode == 'convection_mask':\n","\n","      file_e = glob.glob('/content/gdrive/My Drive/Project/Results/' + Rai + '_' + Raf + '_13_1_19' + '/masks/convection/' + str(tf) + '/arrays/' + str(i) + '_' + '*.npy')\n","      file_list_y = file_list_y + [file_e]\n","\n","  file_list_y = np.ravel(file_list_y)\n","\n","  for file_path in file_list_y:\n","    \n","    a = np.load(file_path, allow_pickle=True)\n","\n","    data_y = data_y + [a]\n","  \n","  return data_x, data_y\n","def swap(i,j):\n","  a = i.swapaxes(0,1)\n","  b = j.swapaxes(0,1)\n","  return a,b\n","\n","def unsqueeze(i,j):\n","  a = torch.unsqueeze(i,0)\n","  b = torch.unsqueeze(j,0)\n","  return a,b\n","def split_x_y_list(data_x, data_y, train_size, seed):\n","\n","\n","  data_x = random.Random(seed).sample(data_x, len(data_x))\n","  data_y = random.Random(seed).sample(data_y, len(data_y))\n","\n","\n","  middle_index = int(np.round(len(data_x) * train_size))\n","\n","  data_x_train = data_x[:middle_index]\n","  data_x_test = data_x[middle_index:]\n","\n","  data_y_train = data_y[:middle_index]\n","  data_y_test = data_y[middle_index:]\n","\n","  return data_x_train, data_y_train, data_x_test, data_y_test\n","def generate_loader(data_x, data_y, TRAIN_SIZE, BATCH_SIZE, SEED):\n","\n","  data_x_train, data_y_train, data_x_test, data_y_test = split_x_y_list(data_x, data_y, TRAIN_SIZE, SEED)\n","\n","  ##########################\n","  ### Dataset\n","  ##########################\n","\n","  import torch\n","  import numpy as np\n","  from torch.utils.data import TensorDataset, DataLoader\n","\n","  import random\n","\n","  my_x_train = np.array(data_x_train)\n","  my_x_test = np.array(data_x_test)\n","\n","  my_y_train = np.array(data_y_train) \n","  my_y_test = np.array(data_y_test)\n","\n","  tensor_x_train = torch.Tensor(my_x_train) # transform to torch tensor\n","  tensor_x_test = torch.Tensor(my_x_test)\n","  tensor_y_train = torch.Tensor(my_y_train) \n","  tensor_y_test = torch.Tensor(my_y_test)\n","\n","  #tensor_x_train, tensor_x_test, tensor_y_train, tensor_y_test = tensor_x_train.to(DEVICE), tensor_x_test.to(DEVICE), tensor_y_train.to(DEVICE), tensor_y_test.to(DEVICE)\n","\n","  train_dataset = TensorDataset(tensor_x_train, tensor_y_train)\n","  test_dataset = TensorDataset(tensor_x_test, tensor_y_test)  \n","\n","  train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE) \n","  test_loader = DataLoader(test_dataset, batch_size = BATCH_SIZE) \n","\n","\n","  train_loader = [unsqueeze(i,j) for (i,j) in train_loader]\n","  train_loader = [swap(i,j) for (i,j) in train_loader]\n","\n","  test_loader = [unsqueeze(i,j) for (i,j) in test_loader]\n","  test_loader = [swap(i,j) for (i,j) in test_loader]\n","\n","  # Checking the dataset\n","  print('Training Set:\\n')\n","  for image_x, image_y in train_loader: \n","      print('Image x batch dimensions:', image_x.size())\n","      print('Image y batch dimensions:', image_y.size())\n","    \n","\n","  # Checking the dataset\n","  print('\\nTesting Set:')\n","  for image_x, image_y in test_loader:\n","      print('Image x batch dimensions:', image_x.size())\n","      print('Image y batch dimensions:', image_y.size())\n","    \n","\n","  return train_loader, test_loader\n","\n","##########################\n","### MODEL\n","##########################\n","\n","\n","class Reshape(nn.Module):\n","    def __init__(self, *args):\n","        super().__init__()\n","        self.shape = args\n","\n","    def forward(self, x):\n","        return x.view(self.shape)\n","\n","\n","class Trim(nn.Module):\n","    def __init__(self, *args):\n","        super().__init__()\n","\n","    def forward(self, x):\n","        return x[:, :, :256, :256]\n","\n","\n","class AutoEncoder(nn.Module):\n","\n","    def __init__(self, m):\n","        super().__init__()\n","        \n","        self.encoder = nn.Sequential( #784\n","                nn.Conv2d(1, 4, stride=(1, 1), kernel_size=(4, 4), padding=0),\n","                nn.LeakyReLU(0.01),\n","                nn.Conv2d(4, 16, stride=(1, 1), kernel_size=(4, 4), padding=0),\n","                nn.LeakyReLU(0.01),\n","                nn.Conv2d(16, 16, stride=(1, 1), kernel_size=(4, 4), padding=0),\n","                nn.LeakyReLU(0.01),\n","                nn.Conv2d(16, 16, stride=(1, 1), kernel_size=(4, 4), padding=0),\n","                nn.Flatten(),\n","                nn.Linear(952576, m)\n","                )\n","\n","        self.decoder = nn.Sequential(\n","                torch.nn.Linear(m, 952576),\n","                Reshape(-1, 16, 244, 244),\n","                nn.ConvTranspose2d(16, 16, stride=(1, 1), kernel_size=(4, 4), padding=0),\n","                nn.LeakyReLU(0.01),\n","                nn.ConvTranspose2d(16, 16, stride=(1, 1), kernel_size=(4, 4), padding=0),                \n","                nn.LeakyReLU(0.01),\n","                nn.ConvTranspose2d(16, 4, stride=(1, 1), kernel_size=(4, 4), padding=0),                \n","                nn.LeakyReLU(0.01),\n","                nn.ConvTranspose2d(4, 1, stride=(1, 1), kernel_size=(4, 4), padding=0), \n","                #Trim(),  # 1x29x29 -> 1x28x28\n","                #nn.Sigmoid()\n","                )\n","\n","    def forward(self, x):\n","        x = self.encoder(x)\n","        x = self.decoder(x)\n","        return x\n","def run_cae(Rai, Raf, ti, tf, mode, TRAIN_SIZE, BATCH_SIZE, SEED, RANDOM_SEED, LEARNING_RATE, NUM_EPOCHS, DEVICE, MIN_M, MAX_M, NUM_M):\n","\n","  data_x, data_y = load_data(Rai, Raf, ti, tf, mode)\n","\n","  train_loader, test_loader = generate_loader(data_x, data_y, TRAIN_SIZE, BATCH_SIZE, SEED)\n","  \n","  #np.random.seed(1)\n","\n","  #m_values = np.random.randint(MIN_M, MAX_M, NUM_M)\n","  #m_values = np.delete(m_values, 3)\n","  #m_values = m_values[-3:]\n","\n","  m_values = np.array([100])\n","\n","  print(m_values)\n","\n","  word = 'word'\n","\n","  for i, m in enumerate(m_values):\n","\n","    m = int(m)\n","\n","    print(m)\n","    print()\n","\n","    if i != 0:\n","\n","      time.sleep(30)\n","\n","      model = model.cpu()\n","      del model\n","      del optimizer\n","      del test_acc\n","      gc.collect()\n","      torch.cuda.empty_cache()\n","\n","      time.sleep(30)\n","\n","\n","    set_all_seeds(RANDOM_SEED)\n","\n","    model = AutoEncoder(m)\n","    model = model.to(DEVICE)\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","  \n","    model = model.cpu()\n","    del model\n","    del optimizer\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","\n","    model = AutoEncoder(m)\n","    model = model.to(DEVICE)\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","\n","\n","    log_dict = train_autoencoder_v1(num_epochs=NUM_EPOCHS, model=model, \n","                                    optimizer=optimizer,\n","                                    train_loader=train_loader, device = DEVICE,\n","                                    skip_epoch_stats=True)\n","  \n","    test_acc = compute_accuracy(model, test_loader, DEVICE)\n","\n","    if mode == 'diffusion_mask':\n","      word = 'diffusion'\n","    elif mode == 'convection_mask':\n","      word = 'convection'\n","    elif mode == 'no_masking':\n","      word = 'base'\n","\n","    os.mkdir('/content/gdrive/My Drive/Project/Results/' + Rai + '_' + Raf + '_' + str(ti) + '_' + str(tf)  + '/models/' + word + '/' + str(m) + '_RS' + str(RANDOM_SEED))\n","\n","    torch.save(model.state_dict(), '/content/gdrive/My Drive/Project/Results/' + Rai + '_' + Raf + '_' + str(ti) + '_' + str(tf)  + '/models/' + word + '/' + str(m) + '_RS' + str(RANDOM_SEED) + '/model.pt')\n","\n","    np.save('/content/gdrive/My Drive/Project/Results/' + Rai + '_' + Raf + '_' + str(ti) + '_' + str(tf)  + '/models/' + word + '/' + str(m) + '_RS' + str(RANDOM_SEED) + '/test_acc.npy', test_acc, allow_pickle=True)\n","    np.save('/content/gdrive/My Drive/Project/Results/' + Rai + '_' + Raf + '_' + str(ti) + '_' + str(tf)  + '/models/' + word + '/' + str(m) + '_RS' + str(RANDOM_SEED) + '/log_dict.npy', log_dict, allow_pickle=True)\n","\n","    print(test_acc)\n","  "]}]}