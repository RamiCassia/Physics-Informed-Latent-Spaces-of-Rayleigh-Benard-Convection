{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Segmenter.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPo8QEA4BfcmJFn7QA9hQEX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Pn9piaFFtRrA"},"outputs":[],"source":["from google.colab import drive\n","drive.mount(\"/content/gdrive\")\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from numpy.random import randint\n","import sklearn as sk\n","from sklearn.mixture import GaussianMixture\n","from sklearn.decomposition import SparsePCA\n","from scipy.io import loadmat\n","import h5py\n","from scipy import sparse, linalg\n","from scipy.optimize import curve_fit, root\n","from scipy.integrate import odeint\n","from scipy.interpolate import interp1d\n","import shutil\n","import os\n","import time\n","\n","plt.rcParams['font.family'] = 'serif'\n","plt.rcParams['font.serif'] = ['Times New Roman'] + plt.rcParams['font.serif']\n","\n","\n","import matplotlib as mpl\n","from matplotlib.colors import ListedColormap\n","\n","# Seaborn colormap\n","import seaborn as sns\n","sns_list = sns.color_palette('deep').as_hex()\n","sns_list.insert(0, '#ffffff')  # Insert white at zero position\n","sns_cmap = ListedColormap(sns_list)\n","\n","cm = sns_cmap\n","\n","mpl_colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728',\n","            '#9467bd', '#8c564b', '#e377c2', '#7f7f7f',\n","            '#bcbd22', '#17becf']\n","\n","from sklearn.datasets import fetch_openml\n","from sklearn.decomposition import PCA\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import StandardScaler\n","\n","import time\n","import os\n","import glob\n","import random\n","import json\n","import subprocess\n","import sys\n","import gc\n","\n","def load_data(Rai, Raf, ti, tf, mode):\n","\n","  file_count = 0\n","\n","  if mode == 'no_masking':\n","\n","    _, _, files = next(os.walk('/content/gdrive/My Drive/Project/Results/' + Rai + '_' + Raf + '_' + str(ti) + '_' + str(tf)  + '/snapshots/' + str(ti) + '_0/arrays'))\n","    file_count = len(files)\n","\n","  elif mode == 'diffusion_mask':\n","\n","    _, _, files = next(os.walk('/content/gdrive/My Drive/Project/Results/' + Rai + '_' + Raf + '_' + str(ti) + '_' + str(tf)  + '/masks/diffusion/' + str(ti) + '_0/arrays'))\n","    file_count = len(files)\n","\n","  elif mode == 'convection_mask':\n","\n","    _, _, files = next(os.walk('/content/gdrive/My Drive/Project/Results/' + Rai + '_' + Raf + '_' + str(ti) + '_' + str(tf)  + '/masks/convection/' + str(ti) + '_0/arrays'))\n","    file_count = len(files)\n","\n","  data_x = []\n","  file_list_x = []\n","\n","  data_y = []\n","  file_list_y = []\n","\n","  for i in range(file_count):\n","\n","    if mode == 'no_masking':\n","\n","      file_e = glob.glob('/content/gdrive/My Drive/Project/Results/' + Rai + '_' + Raf + '_' + str(ti) + '_' + str(tf)  + '/snapshots/' + str(ti) + '_0/arrays/' + str(i) + '_' + '*.npy')\n","      file_list_x = file_list_x + [file_e]\n","\n","    elif mode == 'diffusion_mask':\n","      file_e = glob.glob('/content/gdrive/My Drive/Project/Results/' + Rai + '_' + Raf + '_' + str(ti) + '_' + str(tf)  + '/masks/diffusion/' + str(ti) + '_0/arrays/' + str(i) + '_' + '*.npy')\n","      file_list_x = file_list_x + [file_e]\n","\n","    elif mode == 'convection_mask':\n","      file_e = glob.glob('/content/gdrive/My Drive/Project/Results/' + Rai + '_' + Raf + '_' + str(ti) + '_' + str(tf)  + '/masks/convection/' + str(ti) + '_0/arrays/' + str(i) + '_' + '*.npy')\n","      file_list_x = file_list_x + [file_e]\n","\n","  file_list_x = np.ravel(file_list_x)\n","\n","  for file_path in file_list_x:\n","    \n","    a = np.load(file_path, allow_pickle=True)\n","\n","    data_x = data_x + [a]\n","\n","  for i in range(file_count):\n","\n","    if mode == 'no_masking':\n","\n","      file_e = glob.glob('/content/gdrive/My Drive/Project/Results/' + Rai + '_' + Raf + '_' + str(ti) + '_' + str(tf)  + '/snapshots/' + str(tf) + '_1/arrays/' + str(i) + '_' + '*.npy')\n","      file_list_y = file_list_y + [file_e]\n","\n","    elif mode == 'diffusion_mask':\n","\n","      file_e = glob.glob('/content/gdrive/My Drive/Project/Results/' + Rai + '_' + Raf + '_' + str(ti) + '_' + str(tf)  + '/masks/diffusion/' + str(tf) + '_1/arrays/' + str(i) + '_' + '*.npy')\n","      file_list_y = file_list_y + [file_e]\n","  \n","    elif mode == 'convection_mask':\n","\n","      file_e = glob.glob('/content/gdrive/My Drive/Project/Results/' + Rai + '_' + Raf + '_' + str(ti) + '_' + str(tf)  + '/masks/convection/' + str(tf) + '_1/arrays/' + str(i) + '_' + '*.npy')\n","      file_list_y = file_list_y + [file_e]\n","\n","  file_list_y = np.ravel(file_list_y)\n","\n","  for file_path in file_list_y:\n","    \n","    a = np.load(file_path, allow_pickle=True)\n","\n","    data_y = data_y + [a]\n","  \n","  return data_x, data_y\n","def scientific_to_string(Ra):\n","  Ra_s = format(Ra, '.3e')\n","  Ra_s = Ra_s.replace('.', 'p')\n","  Ra_s = Ra_s.replace('+', '')\n","\n","  return Ra_s\n","\n","\n","def create_time_list(ti, inc, tf):\n","\n","  time_list = [ti]\n","  t = ti\n","\n","  while t < tf:\n","\n","    t = t + inc\n","    time_list = time_list + [t]\n","\n","  return time_list\n","\n","\n","def create_result_dirs(Rai, Raf, ti, inc, tf):\n","\n","  time_list = create_time_list(ti, inc, tf)\n","\n","  Rai_s = scientific_to_string(Rai)\n","  Raf_s = scientific_to_string(Raf)\n","\n","  main_directory = Rai_s + '_' + Raf_s + '_' + str(ti) + '_' + str(inc) + '_' + str(tf)\n","\n","  root_path = '/content/gdrive/My Drive/Project/Results/'\n","\n","  os.mkdir(os.path.join(root_path, main_directory))\n","\n","  snapshots_folder = ['snapshots', 'snapshots/' + str(ti), 'snapshots/' + str(ti) + '/arrays', 'snapshots/' + str(ti) + '/images']\n","\n","  segmentations_folder = ['segmentations', 'segmentations/' + str(ti), 'segmentations/' + str(ti) + '/arrays', 'segmentations/' + str(ti) + '/images']\n","\n","  masks_folder = ['masks', 'masks/diffusion', 'masks/diffusion/' + str(ti), 'masks/diffusion/' + str(ti) + '/arrays', 'masks/diffusion/' + str(ti) + '/images', 'masks/convection' ,'masks/convection/' + str(ti), 'masks/convection/' + str(ti) + '/arrays', 'masks/convection/' + str(ti) + '/images']\n","\n","  models_folder = ['models', 'models/diffusion', 'models/convection', 'models/base']\n","\n","  for t in time_list[1:]:\n","\n","    snapshots_folder = snapshots_folder + ['snapshots/' + str(t), 'snapshots/' + str(t) + '/arrays', 'snapshots/' + str(t) + '/images']\n","\n","    segmentations_folder = segmentations_folder + ['segmentations/' + str(t), 'segmentations/' + str(t) + '/arrays', 'segmentations/' + str(t) + '/images']\n","\n","    masks_folder = masks_folder + ['masks/diffusion/' + str(t), 'masks/diffusion/' + str(t) + '/arrays', 'masks/diffusion/' + str(t) + '/images', 'masks/convection/' + str(t), 'masks/convection/' + str(t) + '/arrays', 'masks/convection/' + str(t) + '/images']\n","\n","  folders = snapshots_folder + segmentations_folder + masks_folder + models_folder\n","\n","  for folder in folders:\n","    \n","    os.mkdir(os.path.join(os.path.join(root_path, main_directory), folder))\n","\n","  return Rai_s, Raf_s\n","\n","\n","def create_Ra_path_list(Rai, Raf):\n","  n = 0\n","  Rai_s = scientific_to_string(Rai)\n","  Raf_s = scientific_to_string(Raf)\n","\n","  Ra_list = []\n","\n","  for subdir in os.scandir('/content/gdrive/My Drive/Project/all_snapshot_groups'):\n","    subdir_name = subdir.name[-8:]\n","    subdir_name = subdir_name.replace('p', '.')\n","    subdir_Ra_num = float(subdir_name)\n","\n","    #count number of data points\n","    if subdir_Ra_num >= Rai and subdir_Ra_num <= Raf:\n","      n += 1\n","\n","    #determine initial index\n","    if Rai_s in subdir.name:\n","      Ra_list = Ra_list + [subdir.name]\n","      start = subdir.name.find(\"snapshots_\") + len(\"snapshots_\")\n","      end = subdir.name.find(subdir.name[-9:])\n","      start_index = int(subdir.name[start:end])\n","  \n","  #append to list\n","  for i in range(start_index + 1, start_index + n):\n","    substring = 'snapshots_' + str(i) + '_'\n","    \n","    for subdir in os.scandir('/content/gdrive/My Drive/Project/all_snapshot_groups'):\n","\n","      if substring in subdir.name:\n","        Ra_list = Ra_list + [subdir.name]\n","\n","\n","  Ra_path_list = Ra_list.copy()   \n","\n","  Ra_path_list = ['/content/gdrive/My Drive/Project/all_snapshot_groups/' + el + '/' + el + '_s1/' + el + '_s1_p0.h5' for el in Ra_path_list]\n","\n","  return Ra_path_list\n","def getGradient(f, zm):\n","  \"\"\"\n","  Calculate the gradients of a field\n","  f        is a matrix of the field\n","  dx       is the cell size\n","  f_dx     is a matrix of derivative of f in the x-direction\n","  f_dz     is a matrix of derivative of f in the z-direction\n","  \"\"\"\n","  # directions for np.roll() \n","  R = -1   # right\n","  L = 1    # left\n","  \n","\n","  f_dz = (np.roll(f,R,axis=0) - np.roll(f,L,axis=0) ) / ( np.roll(zm,R,axis=0) - np.roll(zm,L,axis=0))\n","  f_dz[0,:] = (f[0,:] - f[1,:])/ (zm[0,:] - zm[1,:])\n","  f_dz[-1,:] = (f[-1,:] - f[-2,:])/ (zm[-1,:] - zm[-2,:])\n","\n","  \n","  return f_dz\n","\n","def get_flux_terms(Ta, ua, wa, sn, P, xm, zm):\n","  sn = int(sn)\n","  T_dt = np.flip(((Ta[sn + 1] - Ta[sn - 1]) / 0.2).T)\n","  T = np.flip(Ta[sn].T)\n","  u = np.flip(ua[sn].T)\n","  w = np.flip(wa[sn].T)\n","\n","  T_bar = (np.mean(T, axis = 1))\n","  T_bar = np.reshape(T_bar,(256,1))\n","  T_bar = np.tile(T_bar, (1,256))\n","\n","  wT = np.multiply(w,T)\n","  wT_bar = (np.mean(wT, axis = 1))\n","  wT_bar = np.reshape(wT_bar, (256,1))\n","  wT_bar = np.tile(wT_bar, (1,256))\n","\n","  PT_bar_dz = P * getGradient(T_bar, zm)\n","\n","  return wT_bar, PT_bar_dz, T\n","\n","\n","def create_masks(T, clustermap):\n","  mask_diff = T.copy()\n","  mask_conv = T.copy()\n","\n","  for i in range(256):\n","\n","    if clustermap[i,:].all() != 1:\n","\n","      mask_diff[i, :] = 0\n","\n","    if clustermap[i,:].all() != 0:\n","      mask_conv[i, :] = 0\n","\n","  return mask_diff, mask_conv\n","\n","\n","def create_masks_smooth(T, clustermap, diffusivity=15):\n","  mask_diff = T.copy()\n","  mask_conv = T.copy()\n","  \n","  for i in range(256):\n","\n","    if clustermap[i,:].all() != 1:\n","\n","      count = 1\n","\n","      for n in range(i, 128):\n","\n","        mask_diff[n,:] = (np.absolute(mask_diff[n - 1, :]) - (1/diffusivity)*np.absolute(mask_diff[i, :]))*np.sign(mask_diff[i, :])\n","\n","        count +=1\n","\n","        if count > diffusivity:\n","\n","          mask_diff[n,:] = 0\n","\n","  mask_diff = np.flip(mask_diff)\n","\n","  for i in range(256):\n","\n","    if clustermap[i,:].all() != 1:\n"," \n","      count = 1\n","\n","      for n in range(i, 128):\n","\n","        mask_diff[n,:] = (np.absolute(mask_diff[n - 1, :]) - (1/diffusivity)*np.absolute(mask_diff[i, :]))*np.sign(mask_diff[i, :])\n","\n","        count +=1\n","\n","        if count > diffusivity:\n","\n","          mask_diff[n,:] = 0\n","\n","  mask_diff = np.flip(mask_diff)\n","\n","  mask_conv = T - mask_diff\n","\n","  return mask_diff, mask_conv\n","\n","\n","def segment_and_mask(Ta, ua, wa, ti, inc, tf, Ra, x, z):\n","\n","  time_list = create_time_list(ti, inc, tf)\n","\n","  index_list = [e*10 for e in time_list]\n","\n","  P = 1/(np.sqrt(Ra))\n","\n","  xm, zm = np.meshgrid(x, z)\n","\n","  T_l, clustermap_l, mask_diff_l, mask_conv_l, thickness_l = [], [], [], [], []\n","\n","  labels = [r'$\\overline{wT}$', r'$P \\overline{T}_z$']\n","\n","  thickness_i = 0\n","  thickness_f = 0\n","\n","  for sn in index_list:\n","\n","\n","    wT_bar, PT_bar_dz, T = get_flux_terms(Ta, ua, wa, sn, P, xm, zm)\n","\n","    # Train Gaussian mixture model\n","\n","    features = np.vstack([wT_bar.flatten('F'), PT_bar_dz.flatten('F')]).T\n","    nfeatures = features.shape[1]\n","        \n","    # Fit Gaussian mixture model\n","    nc = 2  # Number of clusters\n","    seed = 5\n","    model = GaussianMixture(n_components=nc, random_state=seed)\n","\n","    model.fit(features[:, :])\n","\n","    # \"Predict\" clusters in entire domain\n","    cluster_idx = model.predict(features)\n","    clustermap = np.reshape(cluster_idx, [len(x), len(z)], order = 'F')\n","  \n","    if clustermap[0,0] == 0:\n","      clustermap = 1 - clustermap\n","      cluster_idx = 1 - cluster_idx\n","\n","    cnt = 0\n","\n","    for i in range(256):\n","      if clustermap[i,0] == 1:\n","        cnt += 1\n","      else:\n","        break\n","\n","    thickness = z[-1] - z[-cnt]\n","    thickness_l = thickness_l + [thickness]    \n","  \n","    clustermap[cnt: (256-cnt)] = np.zeros(256)\n","    clustermap[(256-cnt):] = np.ones(256)\n","    \n","    mask_diff, mask_conv = create_masks_smooth(T, clustermap)\n","\n","    T_l = T_l + [T]\n","    clustermap_l = clustermap_l + [clustermap]\n","    mask_diff_l = mask_diff_l + [mask_diff]\n","    mask_conv_l = mask_conv_l + [mask_conv]\n","\n","\n","    # Colormap of clusters in entire domain\n","    fig, axs = plt.subplots(1, 5, figsize=(25, 4))\n","\n","    axs0 = axs[0].pcolor(x, z,np.flip(T))\n"," \n","    axs1 = axs[1].pcolor(x, z, np.flip(clustermap + 1), cmap=cm, vmin=-0.4, vmax=cm.N-0.4)\n","\n","    axs2 = axs[2].scatter(features[:, 0], features[:,1], 10, cluster_idx + 1, cmap=cm)\n","    axs2.set_clim([-.4, cm.N-0.4])\n","\n","    axs3 = axs[3].pcolor(x, z, np.flip(mask_diff))\n","\n","    axs4 = axs[4].pcolor(x, z, np.flip(mask_conv))\n","\n","    axs[0].tick_params(axis='both', which='major', labelsize=6)\n","    axs[0].tick_params(axis='both', which='minor', labelsize=6)\n","    axs[0].autoscale(enable=True, axis='both', tight=None)\n","\n","    axs[1].tick_params(axis='both', which='major', labelsize=6)\n","    axs[1].tick_params(axis='both', which='minor', labelsize=6)\n","    axs[1].autoscale(enable=True, axis='both', tight=None)\n","\n","    axs[2].tick_params(axis='both', which='major', labelsize=6)\n","    axs[2].tick_params(axis='both', which='minor', labelsize=6)\n","    axs[2].autoscale(enable=True, axis='both', tight=None)\n","\n","    axs[3].tick_params(axis='both', which='major', labelsize=6)\n","    axs[3].tick_params(axis='both', which='minor', labelsize=6)\n","    axs[3].autoscale(enable=True, axis='both', tight=None)\n","\n","    axs[4].tick_params(axis='both', which='major', labelsize=6)\n","    axs[4].tick_params(axis='both', which='minor', labelsize=6)\n","    axs[4].autoscale(enable=True, axis='both', tight=None)\n","\n","\n","    axs[0].set_xlabel('$x$', fontsize=10)\n","    axs[0].set_ylabel('$z$', fontsize=10)\n","    axs[0].set_title('Temperature at t = ' + str(np.round(sn*0.1, 1)) +'s', fontsize=12)\n","\n","    axs[1].set_xlabel('$x$', fontsize=10)\n","    axs[1].set_ylabel('$z$', fontsize=10)\n","    axs[1].set_title('GMM Segmentation', fontsize=12)\n","\n","    axs[2].set_xlabel(labels[0], fontsize=10)\n","    axs[2].set_ylabel(labels[1], fontsize=10)\n","    axs[2].set_title('GMM Clustering', fontsize=12)\n","\n","    axs[3].set_title('Diffusion Mask', fontsize=12)\n","    axs[3].set_xlabel('$x$', fontsize=10)\n","    axs[3].set_ylabel('$z$', fontsize=10)\n","\n","    axs[4].set_title('Convection Mask', fontsize=12)\n","    axs[4].set_xlabel('$x$', fontsize=10)\n","    axs[4].set_ylabel('$z$', fontsize=10)\n","\n","    axs0.set_clim(-0.5, 0.5)\n","    axs3.set_clim(-0.5, 0.5)\n","    axs4.set_clim(-0.5, 0.5)\n","\n","    plt.show()\n","\n","    print()\n","\n","  return T_l, clustermap_l, mask_diff_l, mask_conv_l, thickness_l\n","def generate_data(Rai, Raf, ti, inc, tf):\n","\n","  time_list = create_time_list(ti, inc, tf)\n","\n","  Ra_list = []\n","  thickness_all_list = []\n","\n","  x = np.load('/content/gdrive/My Drive/Project/Other/x256.npy')\n","  z = np.load('/content/gdrive/My Drive/Project/Other/z256.npy')\n","  x = np.ravel(x)\n","  z = np.ravel(z)\n","\n","  Rai_s, Raf_s = create_result_dirs(Rai, Raf, ti, inc, tf)\n","\n","  path_list = create_Ra_path_list(Rai, Raf)\n","\n","  count = 0\n","  \n","  for path in path_list:\n","\n","\n","    f = h5py.File(path)\n","\n","    Ta = f['tasks']['T'][:]\n","    ua = f['tasks']['u'][:]\n","    wa = f['tasks']['w'][:]\n","\n","    Ra = path[-17:-9]\n","    print(count, Ra)\n","    Ra = Ra.replace('p', '.')\n","    Ra = float(Ra)\n","\n","    T_l, clustermap_l, mask_diff_l, mask_conv_l, thickness_l = segment_and_mask(Ta, ua, wa, ti, inc, tf, Ra, x, z)\n","\n","    Ra_list = Ra_list + [Ra]\n","    thickness_all_list = thickness_all_list + [thickness_l]\n","\n","    Ra = path[-17:-9]\n","\n","    for i in range(len(time_list)):\n","\n","      np.save('/content/gdrive/My Drive/Project/Results/' + Rai_s + '_' + Raf_s + '_' + str(ti) + '_' + str(inc) + '_' + str(tf) + '/snapshots/' + str(time_list[i]) + '/arrays/' + str(count) + '_' + Ra + '.npy', T_l[i], allow_pickle = True)\n","      np.save('/content/gdrive/My Drive/Project/Results/' + Rai_s + '_' + Raf_s + '_' + str(ti) + '_' + str(inc) + '_' + str(tf) + '/segmentations/' + str(time_list[i]) + '/arrays/' + str(count) + '_' + Ra + '.npy', clustermap_l[i], allow_pickle = True)\n","      np.save('/content/gdrive/My Drive/Project/Results/' + Rai_s + '_' + Raf_s + '_' + str(ti) + '_' + str(inc) + '_' + str(tf) + '/masks/' + 'diffusion/' + str(time_list[i]) + '/arrays/' + str(count) + '_' + Ra + '.npy', mask_diff_l[i], allow_pickle = True)\n","      np.save('/content/gdrive/My Drive/Project/Results/' + Rai_s + '_' + Raf_s + '_' + str(ti) + '_' + str(inc) + '_' + str(tf) + '/masks/' + 'convection/' + str(time_list[i]) + '/arrays/' + str(count) + '_' + Ra + '.npy', mask_conv_l[i], allow_pickle = True)\n","    \n","    count += 1\n","\n","    np.save('/content/gdrive/My Drive/Project/Results/' + Rai_s + '_' + Raf_s + '_' + str(ti) + '_' + str(inc) + '_' + str(tf) + '/Ra_list.npy', Ra_list)\n","    np.save('/content/gdrive/My Drive/Project/Results/' + Rai_s + '_' + Raf_s + '_' + str(ti) + '_' + str(inc) + '_' + str(tf) + '/thickness_list.npy', thickness_all_list)\n","\n","\n","    \n","  return Ra_list, thickness_all_list\n"]}]}